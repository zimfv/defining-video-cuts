{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c181a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 03:21:38.512975: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-22 03:21:38.551173: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-22 03:21:38.551216: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-22 03:21:38.551255: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-22 03:21:38.558338: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-22 03:21:38.558850: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-22 03:21:39.342192: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import imageio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e8557e",
   "metadata": {},
   "source": [
    "# Build a Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f190f3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GifIterator:\n",
    "    def __init__(self, filenames, answers, batch_size=64, rescale=None, shuffle=True):\n",
    "        \"\"\"\n",
    "        Yields batches (X, y)\n",
    "        \"\"\"\n",
    "        self.filenames = np.array(filenames)\n",
    "        self.answers = np.array(answers)\n",
    "        self.batch_size = batch_size\n",
    "        self.rescale = rescale\n",
    "        if len(self.filenames) != len(self.answers):\n",
    "            raise ValueError(\"filenames and answers arrays have different lengths.\")\n",
    "        self.length = (len(self.answers) - 1)//self.batch_size + 1\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_index = 0\n",
    "        self.indexes = np.arange(len(self.answers))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    \n",
    "    def reshuffle(self):\n",
    "        self.indexes = np.arange(len(self.answers))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        self.batch_index = 0\n",
    "        self.reshuffle()\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise ValueError(\n",
    "                \"Asked to retrieve element {idx}, \"\n",
    "                \"but the Sequence \"\n",
    "                \"has length {length}\".format(idx=idx, length=len(self))\n",
    "            )\n",
    "        \n",
    "        if idx == len(self) - 1:\n",
    "            indexes_i = self.indexes[idx*self.batch_size:]\n",
    "        else:\n",
    "            indexes_i = self.indexes[idx*self.batch_size:(idx + 1)*self.batch_size]\n",
    "        filenames_i = self.filenames[indexes_i]\n",
    "        answers_i = self.answers[indexes_i]\n",
    "        X = np.array([np.array(imageio.mimread(filename)) for filename in filenames_i])\n",
    "        if self.rescale is not None:\n",
    "            X = X*self.rescale\n",
    "        y = answers_i\n",
    "        return X, y\n",
    "    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def __next__(self):\n",
    "        self.batch_index += 1\n",
    "        if self.batch_index > len(self):\n",
    "            self.on_epoch_end()\n",
    "            self.batch_index = 0  # Reset batch index for the new epoch\n",
    "            raise StopIteration\n",
    "        return self[self.batch_index - 1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1dfba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GifBinaryGenerator:\n",
    "    def __init__(self, batch_size=64, rescale=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.rescale = rescale\n",
    "        self.droped = []\n",
    "    \n",
    "    def prepare_by_paths(self, path_class_0=None, path_class_1=None, check_length=True, gif_length=4):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        -----------\n",
    "        path_class_0 : str\n",
    "            The way to class, label 0\n",
    "            \n",
    "        path_class_1 : str\n",
    "            The way to class, label 1\n",
    "            \n",
    "        check_length : bool\n",
    "            Drop files if their length not correct\n",
    "            \n",
    "        gif_length : int\n",
    "            Ignoring gifs with another lengths\n",
    "        \"\"\"\n",
    "        if check_length:\n",
    "            files0 = []\n",
    "            print('Checking GIF-files for correct length:')\n",
    "            pbar = tqdm(total=len(os.listdir(path_class_0)) + len(os.listdir(path_class_1)))\n",
    "            for file in [os.path.join(path_class_0, filename) for filename in os.listdir(path_class_0)]:\n",
    "                X = np.array(imageio.mimread(file))\n",
    "                if len(X) == gif_length:\n",
    "                    files0.append(file)\n",
    "                else:\n",
    "                    self.droped.append(file)\n",
    "                pbar.update()\n",
    "            files1 = []\n",
    "            for file in [os.path.join(path_class_1, filename) for filename in os.listdir(path_class_1)]:\n",
    "                X = np.array(imageio.mimread(file))\n",
    "                if len(X) == gif_length:\n",
    "                    files1.append(file)\n",
    "                else:\n",
    "                    self.droped.append(file)\n",
    "                pbar.update()\n",
    "            pbar.close()\n",
    "        else:\n",
    "            files0 = [os.path.join(path_class_0, filename) for filename in os.listdir(path_class_0)]\n",
    "            files1 = [os.path.join(path_class_1, filename) for filename in os.listdir(path_class_1)]\n",
    "        self.files0 = np.array(files0)\n",
    "        self.files1 = np.array(files1)\n",
    "    \n",
    "    \n",
    "    def flow(self, path_class_0=None, path_class_1=None, shuffle=True, check_length=True, gif_length=4):\n",
    "        \"\"\"\n",
    "        Yields tuples (X, y) of np.arrays shapes \n",
    "        (self.batch_size, gif_length, width, height, 3) and (self.batch_size)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        path_class_0 : str\n",
    "            The way to class, label 0\n",
    "            \n",
    "        path_class_1 : str\n",
    "            The way to class, label 1\n",
    "            \n",
    "        check_length : bool\n",
    "            Drop files if their length not correct\n",
    "            \n",
    "        gif_length : int\n",
    "            Ignoring gifs with another lengths\n",
    "            \n",
    "        \"\"\"\n",
    "        if (path_class_0 is not None) and (path_class_1 is not None):\n",
    "            self.prepare_by_paths(path_class_0, path_class_1, check_length, gif_length)\n",
    "        \n",
    "        files = np.concatenate([self.files0, self.files1])\n",
    "        answers = np.concatenate([np.zeros(len(self.files0)), np.ones(len(self.files1))])\n",
    "        return GifIterator(files, answers, batch_size=self.batch_size, rescale=self.rescale, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db48769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking GIF-files for correct length:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575098ccb23c4113b7c4dc469024d76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 7 validation files droping:\n",
      "- data/validation/cuts/1-140.gif\n",
      "- data/validation/cuts/1-141.gif\n",
      "- data/validation/noncuts/0-2161.gif\n",
      "- data/validation/noncuts/0-2166.gif\n",
      "- data/validation/noncuts/1-1769.gif\n",
      "- data/validation/noncuts/1-2015.gif\n",
      "- data/validation/noncuts/1-2017.gif\n"
     ]
    }
   ],
   "source": [
    "path_class_testing_0 = 'data/validation/noncuts'\n",
    "path_class_testing_1 = 'data/validation/cuts'\n",
    "\n",
    "gen_validation = GifBinaryGenerator(batch_size=16, rescale=1/255)\n",
    "gen_validation.prepare_by_paths(path_class_testing_0, path_class_testing_1)\n",
    "\n",
    "print(f'\\nThere are {len(gen_validation.droped)} validation files droping:\\n- ' + '\\n- '.join(np.sort(gen_validation.droped)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e5ea7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking GIF-files for correct length:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7344ae0ae35e432bb5e9cac6834b2768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 38 validation files droping:\n",
      "- data/training/cuts/0-150.gif\n",
      "- data/training/cuts/0-35.gif\n",
      "- data/training/cuts/1-139.gif\n",
      "- data/training/cuts/1-142.gif\n",
      "- data/training/cuts/1-20.gif\n",
      "- data/training/cuts/1-219.gif\n",
      "- data/training/cuts/1-233.gif\n",
      "- data/training/cuts/1-234.gif\n",
      "- data/training/cuts/a-151.gif\n",
      "- data/training/cuts/a-20.gif\n",
      "- data/training/cuts/a-217.gif\n",
      "- data/training/cuts/a-278.gif\n",
      "- data/training/cuts/a-521.gif\n",
      "- data/training/cuts/a-537.gif\n",
      "- data/training/cuts/a-676.gif\n",
      "- data/training/cuts/a-691.gif\n",
      "- data/training/noncuts/0-2159.gif\n",
      "- data/training/noncuts/0-2160.gif\n",
      "- data/training/noncuts/0-2162.gif\n",
      "- data/training/noncuts/0-2163.gif\n",
      "- data/training/noncuts/0-2164.gif\n",
      "- data/training/noncuts/0-2165.gif\n",
      "- data/training/noncuts/1-1028.gif\n",
      "- data/training/noncuts/1-1029.gif\n",
      "- data/training/noncuts/1-2003.gif\n",
      "- data/training/noncuts/1-2004.gif\n",
      "- data/training/noncuts/1-2005.gif\n",
      "- data/training/noncuts/1-2006.gif\n",
      "- data/training/noncuts/1-2007.gif\n",
      "- data/training/noncuts/1-2008.gif\n",
      "- data/training/noncuts/1-2009.gif\n",
      "- data/training/noncuts/1-2010.gif\n",
      "- data/training/noncuts/1-2011.gif\n",
      "- data/training/noncuts/1-2012.gif\n",
      "- data/training/noncuts/1-2013.gif\n",
      "- data/training/noncuts/1-2014.gif\n",
      "- data/training/noncuts/1-2016.gif\n",
      "- data/training/noncuts/1-267.gif\n"
     ]
    }
   ],
   "source": [
    "path_class_training_0 = 'data/training/noncuts'\n",
    "path_class_training_1 = 'data/training/cuts'\n",
    "\n",
    "gen_training = GifBinaryGenerator(batch_size=32, rescale=1/255)\n",
    "gen_training.prepare_by_paths(path_class_training_0, path_class_training_1)\n",
    "\n",
    "print(f'\\nThere are {len(gen_training.droped)} validation files droping:\\n- ' + '\\n- '.join(np.sort(gen_training.droped)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c9a11",
   "metadata": {},
   "source": [
    "GIF format sometimes contains not every frame. \n",
    "\n",
    "This is another reason to build generator from DataFrame and video, described in `Data Extraction and Augmentation.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcc60d1",
   "metadata": {},
   "source": [
    "# Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed212b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv3D(16, 3, activation='relu', input_shape=(4, 160, 90, 3)), \n",
    "    tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2)), \n",
    "    tf.keras.layers.Conv3D(32, 2, activation='relu'), \n",
    "    tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2)), \n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'), \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f894b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 2, 158, 88, 16)    1312      \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3  (None, 2, 79, 44, 16)     0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 1, 78, 43, 32)     4128      \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPoolin  (None, 1, 39, 21, 32)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 26208)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               3354752   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3360321 (12.82 MB)\n",
      "Trainable params: 3360321 (12.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f07815af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "140/140 [==============================] - 140s 996ms/step - loss: 0.1960 - accuracy: 0.9404 - val_loss: 0.1272 - val_accuracy: 0.9549\n",
      "Epoch 2/4\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 560 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 57 batches). You may need to use the repeat() function when building your dataset.\n",
      "140/140 [==============================] - 0s 212us/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(gen_training.flow(),\n",
    "                    epochs=4,\n",
    "                    verbose=1,\n",
    "                    validation_data=gen_validation.flow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8249aa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.19600094854831696, 0.0],\n",
       " 'accuracy': [0.9404255151748657, 0.0],\n",
       " 'val_loss': [0.12719303369522095],\n",
       " 'val_accuracy': [0.9548954963684082]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02af3526",
   "metadata": {},
   "source": [
    "Generator returns Iterator, like ImageDataGenerator. But that dies after 1st epoch. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6d6be57",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert len(history.history['val_loss']) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94397b6f",
   "metadata": {},
   "source": [
    "# Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97d0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843aa9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "now = str(now)[:16]\n",
    "\n",
    "print(f'Current date and timie is {now}')\n",
    "now = now.replace(' ', '-').replace(':', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a212839",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = 'models'\n",
    "path_history = 'histories'\n",
    "if not os.path.isdir(path_model):\n",
    "    os.mkdir(path_model)\n",
    "if not os.path.isdir(path_history):\n",
    "    os.mkdir(path_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d1a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = os.path.join(path_model, f'{now}.keras')\n",
    "path_history = os.path.join(path_history, f'{now}.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12183887",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_history, 'wb') as file:\n",
    "    pickle.dump(path_history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b643b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e6574a",
   "metadata": {},
   "source": [
    "# Learning History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b5f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
